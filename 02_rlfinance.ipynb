{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "475819a4-e148-4616-b1cb-44b659aeb08a",
      "metadata": {
        "id": "475819a4-e148-4616-b1cb-44b659aeb08a"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e",
      "metadata": {
        "id": "280cc0c6-2c18-46cd-8af7-3f19b64a6d7e"
      },
      "source": [
        "# Reinforcement Learning for Finance\n",
        "\n",
        "**Chapter 02 &mdash; Deep Q-Learning**\n",
        "\n",
        "&copy; Dr. Yves J. Hilpisch\n",
        "\n",
        "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77506e6-eb5f-4245-921f-061441a16346",
      "metadata": {
        "id": "f77506e6-eb5f-4245-921f-061441a16346"
      },
      "source": [
        "### Please use the \"Python 3.10, Tensorflow 2.10\" kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6be6f8b-e00e-402c-9df1-1d3f16e76c7e",
      "metadata": {
        "id": "d6be6f8b-e00e-402c-9df1-1d3f16e76c7e"
      },
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e3924c3-2cad-4400-8806-5acf2f4b9b16",
      "metadata": {
        "id": "5e3924c3-2cad-4400-8806-5acf2f4b9b16"
      },
      "source": [
        "### The Game Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a49941f9",
      "metadata": {
        "id": "a49941f9"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tpq-classes/rl_4_finance.git\n",
        "import sys\n",
        "sys.path.append('rl_4_finance')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f3a51a-71e6-497d-bab3-926444a6bb30",
      "metadata": {
        "id": "72f3a51a-71e6-497d-bab3-926444a6bb30"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e19725f2-a026-487e-826c-00fa5fce71ec",
      "metadata": {
        "id": "e19725f2-a026-487e-826c-00fa5fce71ec"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af76fb4e-3b31-4465-bff5-e5f8362af3d2",
      "metadata": {
        "id": "af76fb4e-3b31-4465-bff5-e5f8362af3d2"
      },
      "outputs": [],
      "source": [
        "env.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb45da1-6f9c-464d-bb16-e098ddd52838",
      "metadata": {
        "id": "bdb45da1-6f9c-464d-bb16-e098ddd52838"
      },
      "outputs": [],
      "source": [
        "env.action_space.n  # <1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e8ec50-f5a4-4706-8937-6724582ebdc3",
      "metadata": {
        "id": "77e8ec50-f5a4-4706-8937-6724582ebdc3"
      },
      "outputs": [],
      "source": [
        "[env.action_space.sample() for _ in range(10)]  # <1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "592d3ddc-3958-42ff-b4c7-8924ce0a343d",
      "metadata": {
        "id": "592d3ddc-3958-42ff-b4c7-8924ce0a343d"
      },
      "outputs": [],
      "source": [
        "env.observation_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19474f1a-29c3-4cc2-89f6-6226845f5468",
      "metadata": {
        "id": "19474f1a-29c3-4cc2-89f6-6226845f5468"
      },
      "outputs": [],
      "source": [
        "env.observation_space.shape  # <2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bdd054d-4a5e-429e-9e44-3e436a20446d",
      "metadata": {
        "id": "4bdd054d-4a5e-429e-9e44-3e436a20446d"
      },
      "outputs": [],
      "source": [
        "env.reset(seed=100)  # <1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "875c67b7-4817-4fac-8fbb-0596c399af96",
      "metadata": {
        "id": "875c67b7-4817-4fac-8fbb-0596c399af96"
      },
      "outputs": [],
      "source": [
        "env.step(0)  # <2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be7afb1-e69d-41d7-b869-c73747e38b61",
      "metadata": {
        "id": "7be7afb1-e69d-41d7-b869-c73747e38b61"
      },
      "outputs": [],
      "source": [
        "env.step(1)  # <2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f6e49b-3308-418a-999c-f7d6a052cfea",
      "metadata": {
        "id": "f8f6e49b-3308-418a-999c-f7d6a052cfea"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "    def play(self, episodes=1):\n",
        "        self.trewards = list()\n",
        "        for e in range(episodes):\n",
        "            self.env.reset()\n",
        "            for step in range(1, 100):\n",
        "                a = self.env.action_space.sample()\n",
        "                state, reward, done, trunc, info = self.env.step(a)\n",
        "                if done:\n",
        "                    self.trewards.append(step)\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffbb689-b81e-48cc-9fac-3a7dec9c1ff7",
      "metadata": {
        "id": "dffbb689-b81e-48cc-9fac-3a7dec9c1ff7"
      },
      "outputs": [],
      "source": [
        "ra = RandomAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb3b03c-ded1-4ca7-80d2-e316635379b8",
      "metadata": {
        "id": "cbb3b03c-ded1-4ca7-80d2-e316635379b8"
      },
      "outputs": [],
      "source": [
        "ra.play(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b83a7c9-485a-433d-b637-9ffbe6fe7146",
      "metadata": {
        "id": "5b83a7c9-485a-433d-b637-9ffbe6fe7146"
      },
      "outputs": [],
      "source": [
        "ra.trewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d9d910-4f2d-4d7b-bcaa-a28747474c00",
      "metadata": {
        "id": "27d9d910-4f2d-4d7b-bcaa-a28747474c00"
      },
      "outputs": [],
      "source": [
        "round(sum(ra.trewards) / len(ra.trewards), 2)  # <1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e1594d-ea7c-49e9-9149-92848ba72440",
      "metadata": {
        "id": "12e1594d-ea7c-49e9-9149-92848ba72440"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "warnings.simplefilter('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from collections import deque\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0264fac6-2c4a-4ea3-9031-e5006dce93c4",
      "metadata": {
        "id": "0264fac6-2c4a-4ea3-9031-e5006dce93c4"
      },
      "outputs": [],
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.0005)  # <2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c28ee7-4be2-459c-8e27-029ec6ff4b4d",
      "metadata": {
        "id": "e7c28ee7-4be2-459c-8e27-029ec6ff4b4d"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3be08b60-b90a-4585-8e7a-d2e16ce3c8a1",
      "metadata": {
        "id": "3be08b60-b90a-4585-8e7a-d2e16ce3c8a1"
      },
      "outputs": [],
      "source": [
        "# deque?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072e8f75-0936-434f-ad65-c2f7cff91b7c",
      "metadata": {
        "id": "072e8f75-0936-434f-ad65-c2f7cff91b7c"
      },
      "outputs": [],
      "source": [
        "class DQLAgent:\n",
        "    def __init__(self):\n",
        "        self.epsilon = 1.0  # <1>\n",
        "        self.epsilon_decay = 0.9975  # <2>\n",
        "        self.epsilon_min = 0.1  # <3>\n",
        "        self.memory = list()  # <4>\n",
        "        # self.memory = deque(maxlen=2000)\n",
        "        self.batch_size = 32  # <5>\n",
        "        self.gamma = 0.9  # <6>\n",
        "        self.trewards = deque(maxlen=2000)  # <7>\n",
        "        self.max_treward = 0  # <8>\n",
        "        self._create_model()  # <9>\n",
        "        self.env = gym.make('CartPole-v1')  # <10>\n",
        "    def _create_model(self):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(24, activation='relu', input_dim=4))\n",
        "        self.model.add(Dense(24, activation='relu'))\n",
        "        self.model.add(Dense(2, activation='linear'))\n",
        "        self.model.compile(loss='mse', optimizer=opt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Old verson, really slow\n",
        "#class DQLAgent(DQLAgent):\n",
        "#    def act(self, state):\n",
        "#        if random.random() < self.epsilon:\n",
        "#            return self.env.action_space.sample()  # <1>\n",
        "#        return np.argmax(self.model.predict(state)[0])  # <2>\n",
        "#    def replay(self):\n",
        "#        batch = random.sample(self.memory, self.batch_size)  # <3>\n",
        "#        for state, action, next_state, reward, done in batch:\n",
        "#            if not done:\n",
        "#                reward += self.gamma * np.amax(\n",
        "#                    self.model.predict(next_state)[0])  # <4>\n",
        "#            target = self.model.predict(state)  # <5>\n",
        "#            target[0, action] = reward  # <6>\n",
        "#            self.model.fit(state, target, epochs=2, verbose=False)  # <7>\n",
        "#        if self.epsilon > self.epsilon_min:\n",
        "#            self.epsilon *= self.epsilon_decay  # <8>"
      ],
      "metadata": {
        "id": "87306f53"
      },
      "id": "87306f53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "020d5de9",
      "metadata": {
        "auto_refactor_role": "generated",
        "id": "020d5de9"
      },
      "outputs": [],
      "source": [
        "# New Version, much faster\n",
        "class DQLAgent(DQLAgent):\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()  # <1>\n",
        "        q = self.model(tf.convert_to_tensor(state, dtype=tf.float32), training=False)\n",
        "        return int(tf.argmax(q[0]).numpy())\n",
        "\n",
        "\n",
        "    def replay(self):\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        states      = np.vstack([b[0] for b in batch]).astype(np.float32)\n",
        "        actions     = np.array([b[1] for b in batch], dtype=np.int32)\n",
        "        next_states = np.vstack([b[2] for b in batch]).astype(np.float32)\n",
        "        rewards     = np.array([b[3] for b in batch], dtype=np.float32)\n",
        "        dones       = np.array([b[4] for b in batch], dtype=np.bool_)\n",
        "\n",
        "        # Q(s, :)\n",
        "        q_states = self.model(states, training=False).numpy()          # (B, A)\n",
        "\n",
        "        # max_a' Q(s', a')\n",
        "        q_next = self.model(next_states, training=False).numpy()       # (B, A)\n",
        "        max_q_next = np.max(q_next, axis=1)                            # (B,)\n",
        "\n",
        "        targets = q_states.copy()\n",
        "        targets[np.arange(self.batch_size), actions] = rewards + self.gamma * max_q_next * (~dones)\n",
        "\n",
        "         # One update only (FAST)\n",
        "        self.model.train_on_batch(states, targets)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf59f89-41a4-4f6e-8635-0513b3c3d8c1",
      "metadata": {
        "id": "2bf59f89-41a4-4f6e-8635-0513b3c3d8c1"
      },
      "outputs": [],
      "source": [
        "class DQLAgent(DQLAgent):\n",
        "    def learn(self, episodes):\n",
        "        for e in range(1, episodes + 1):\n",
        "            state, _ = self.env.reset()  # <1>\n",
        "            state = np.reshape(state, [1, 4])  # <2>\n",
        "            for f in range(1, 5000):\n",
        "                action = self.act(state)  # <3>\n",
        "                next_state, reward, done, trunc, _ = self.env.step(action)  # <4>\n",
        "                next_state = np.reshape(next_state, [1, 4])  # <2>\n",
        "                self.memory.append(\n",
        "                    [state, action, next_state, reward, done])  # <4>\n",
        "                state = next_state  # <5>\n",
        "                if done or trunc:\n",
        "                    self.trewards.append(f)  # <6>\n",
        "                    self.max_treward = max(self.max_treward, f)  # <7>\n",
        "                    templ = f'episode={e:4d} | treward={f:4d}'\n",
        "                    templ += f' | max={self.max_treward:4d}'\n",
        "                    print(templ, end='\\r')\n",
        "                    break\n",
        "            if len(self.memory) > self.batch_size:\n",
        "                self.replay()  # <8>\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Old Version, really slow\n",
        "#class DQLAgent(DQLAgent):\n",
        "#    def test(self, episodes):\n",
        "#        for e in range(1, episodes + 1):\n",
        "#            state, _ = self.env.reset()\n",
        "#            state = np.reshape(state, [1, 4])\n",
        "#            for f in range(1, 5001):\n",
        "#                action = np.argmax(self.model.predict(state)[0])  # <1>\n",
        "#                state, reward, done, trunc, _ = self.env.step(action)\n",
        "#                state = np.reshape(state, [1, 4])\n",
        "#                if done or trunc:\n",
        "#                    print(f, end=' ')\n",
        "#                    break"
      ],
      "metadata": {
        "id": "696d1391"
      },
      "id": "696d1391",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5532f37c",
      "metadata": {
        "auto_refactor_role": "generated",
        "id": "5532f37c"
      },
      "outputs": [],
      "source": [
        "class DQLAgent(DQLAgent):\n",
        "    def test(self, episodes):\n",
        "        for e in range(1, episodes + 1):\n",
        "            state, _ = self.env.reset()\n",
        "            state = np.reshape(state, [1, 4])\n",
        "            for f in range(1, 5001):\n",
        "                action = np.argmax(self.model(tf.convert_to_tensor(state, dtype=tf.float32), training=False).numpy()[0])  # <1>\n",
        "                state, reward, done, trunc, _ = self.env.step(action)\n",
        "                state = np.reshape(state, [1, 4])\n",
        "                if done or trunc:\n",
        "                    print(f, end=' ')\n",
        "                    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64417ca0-49ba-4558-8c92-d89604ff3e16",
      "metadata": {
        "id": "64417ca0-49ba-4558-8c92-d89604ff3e16"
      },
      "outputs": [],
      "source": [
        "agent = DQLAgent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f77a72ab-5a4b-4d3d-863a-f8d08d2e3ce2",
      "metadata": {
        "tags": [],
        "id": "f77a72ab-5a4b-4d3d-863a-f8d08d2e3ce2"
      },
      "outputs": [],
      "source": [
        "%time agent.learn(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbfc1255-66fe-4c69-9135-70100b981109",
      "metadata": {
        "id": "fbfc1255-66fe-4c69-9135-70100b981109"
      },
      "outputs": [],
      "source": [
        "agent.epsilon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af72f8d3-4e2a-4d0f-8311-a56ba4487832",
      "metadata": {
        "id": "af72f8d3-4e2a-4d0f-8311-a56ba4487832"
      },
      "outputs": [],
      "source": [
        "agent.test(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55",
      "metadata": {
        "id": "20e3eaa7-ac35-44e5-bffc-93662c2d2c55"
      },
      "source": [
        "<img src=\"http://hilpisch.com/tpq_logo.png\" alt=\"The Python Quants\" width=\"35%\" align=\"right\" border=\"0\"><br>\n",
        "\n",
        "<a href=\"http://tpq.io\" target=\"_blank\">http://tpq.io</a> | <a href=\"http://twitter.com/dyjh\" target=\"_blank\">@dyjh</a> | <a href=\"mailto:team@tpq.io\">team@tpq.io</a>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}